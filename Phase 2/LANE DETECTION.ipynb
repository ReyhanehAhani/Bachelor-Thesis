{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":null,"source":["import os\r\n","import cv2\r\n","from tqdm import tqdm\r\n","import torch\r\n","import numpy as np\r\n","from pathlib import Path\r\n","import matplotlib.pyplot as plt\r\n","from torch.utils.data import Dataset, DataLoader\r\n","import torch.nn as nn\r\n","import albumentations as A\r\n","from albumentations.pytorch import ToTensorV2\r\n","import torchvision\r\n","\r\n","!pip install pytorch_warmup torchview\r\n","import pytorch_warmup as warmup\r\n","from torchview import draw_graph"],"outputs":[],"metadata":{"_uuid":"29cb1677-0a2d-4f78-9518-839c0bede192","_cell_guid":"e5a7904a-181f-47fb-bfbc-0c4586632cf8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:45.168259Z","iopub.execute_input":"2023-07-16T16:24:45.168669Z","iopub.status.idle":"2023-07-16T16:24:58.825823Z","shell.execute_reply.started":"2023-07-16T16:24:45.168636Z","shell.execute_reply":"2023-07-16T16:24:58.824735Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["WIDTH = 1640 // 5 #1280 // 5 #1640 // 5 #\r\n","HEIGHT = 590 // 5 #720 // 5 #590 // 5 #\r\n","LEARNING_RATE = 0.001\r\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n","BATCH_SIZE = 64\r\n","NUM_EPOCHS = 20\r\n","NUM_WORKERS = 2\r\n","PIN_MEMORY = True\r\n","DILATION = np.ones((15, 15), np.uint8)"],"outputs":[],"metadata":{"_uuid":"9f0354c0-bee3-47e3-80f0-7da3cb0b3f62","_cell_guid":"740fd54f-5e22-4c15-8847-7c5fa43706b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:58.827627Z","iopub.execute_input":"2023-07-16T16:24:58.828022Z","iopub.status.idle":"2023-07-16T16:24:58.867511Z","shell.execute_reply.started":"2023-07-16T16:24:58.827986Z","shell.execute_reply":"2023-07-16T16:24:58.866509Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["class CULane(Dataset):\r\n","    def __init__(self, image_dir, mask_dir, transform = None):\r\n","        self.images = sorted(tuple(Path(image_dir).rglob('*.jpg')))\r\n","        self.masks = sorted(tuple(Path(mask_dir).rglob('*.png')))\r\n","        self.samples = len(self.images)\r\n","        \r\n","        self.transform = transform\r\n","\r\n","    def __len__(self):\r\n","        return self.samples\r\n","    \r\n","    def resize_with_pad(self, image, new_shape) -> np.array:\r\n","        original_shape = (image.shape[1], image.shape[0])\r\n","        ratio = float(max(new_shape))/max(original_shape)\r\n","        new_size = tuple([int(x*ratio) for x in original_shape])\r\n","        image = cv2.resize(image, new_size)\r\n","        delta_w = new_shape[0] - new_size[0]\r\n","        delta_h = new_shape[1] - new_size[1]\r\n","        top, bottom = delta_h//2, delta_h-(delta_h//2)\r\n","        left, right = delta_w//2, delta_w-(delta_w//2)\r\n","        image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\r\n","        return image\r\n","\r\n","    def __getitem__(self, i):\r\n","        image =  cv2.imread(str(self.images[i]))\r\n","        mask = cv2.imread(str(self.masks[i]), 0)\r\n","        mask = cv2.dilate(mask, DILATION)\r\n","        \r\n","        image = self.resize_with_pad(image, (WIDTH, HEIGHT))\r\n","        mask = self.resize_with_pad(mask, (WIDTH, HEIGHT))\r\n","        \r\n","        mask = mask.astype(np.float32) / 255.0\r\n","        mask[mask > 0] = 1\r\n","        \r\n","        if self.transform:\r\n","            result = self.transform(image=image, mask=mask)\r\n","            image = result['image']\r\n","            mask = result['mask']\r\n","\r\n","        return image, mask"],"outputs":[],"metadata":{"_uuid":"089d9afe-9b78-427a-ab0c-751fc4d1359f","_cell_guid":"a1e37071-5160-499c-be56-54d81251f8c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:58.869349Z","iopub.execute_input":"2023-07-16T16:24:58.870259Z","iopub.status.idle":"2023-07-16T16:24:58.884619Z","shell.execute_reply.started":"2023-07-16T16:24:58.870223Z","shell.execute_reply":"2023-07-16T16:24:58.883644Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["train_transforms = A.Compose([\r\n","    A.Rotate(limit=45, p=0.5),\r\n","    A.HorizontalFlip(p=0.5),\r\n","    A.VerticalFlip(p=0.5),\r\n","    A.CoarseDropout(max_holes=40, min_holes=25, \r\n","                    p=0.5,\r\n","                    max_height=30, \r\n","                    max_width=30, fill_value=1),\r\n","    A.Normalize(\r\n","        mean=[0.0, 0.0, 0.0],\r\n","        std=[1.0, 1.0, 1.0],\r\n","        max_pixel_value=255.0\r\n","    ),\r\n","    ToTensorV2(),\r\n","])\r\n","\r\n","val_transforms = A.Compose([\r\n","    A.Normalize(\r\n","        mean=[0.0, 0.0, 0.0],\r\n","        std=[1.0, 1.0, 1.0],\r\n","        max_pixel_value=255.0,\r\n","    ),\r\n","    ToTensorV2(),\r\n","])"],"outputs":[],"metadata":{"_uuid":"3447b63c-a92d-485d-81cc-7389543a3300","_cell_guid":"b857db21-b3a5-4b3d-b3ca-63305c075b93","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:58.889373Z","iopub.execute_input":"2023-07-16T16:24:58.890046Z","iopub.status.idle":"2023-07-16T16:24:58.900531Z","shell.execute_reply.started":"2023-07-16T16:24:58.890014Z","shell.execute_reply":"2023-07-16T16:24:58.899342Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["dataset_path = '/kaggle/input/culane/CULane'\r\n","full_dataset = CULane(image_dir='/kaggle/input/culane/driver_161_90frame', \r\n","                      mask_dir='/kaggle/input/culane/driver_161_90frame_labels', \r\n","                      transform=None)\r\n","\r\n","train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [0.7, 0.3])\r\n","train_dataset.dataset.transform = train_transforms\r\n","val_dataset.dataset.transform = val_transforms\r\n","\r\n","loader_args = {\r\n","    \"batch_size\": BATCH_SIZE,\r\n","    \"num_workers\": os.cpu_count(), \r\n","    \"pin_memory\": PIN_MEMORY,\r\n","}\r\n","\r\n","train_loader = DataLoader(train_dataset, shuffle=True, **loader_args)\r\n","val_loader = DataLoader(val_dataset, **loader_args)"],"outputs":[],"metadata":{"_uuid":"1a77a495-b590-4488-8a36-4f78d47d6ff6","_cell_guid":"9cd00835-047a-432b-976a-15fb19a00c22","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:58.903424Z","iopub.execute_input":"2023-07-16T16:24:58.904147Z","iopub.status.idle":"2023-07-16T16:24:59.966983Z","shell.execute_reply.started":"2023-07-16T16:24:58.904119Z","shell.execute_reply":"2023-07-16T16:24:59.965757Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["for x, y in train_loader:\r\n","    y = y.unsqueeze(1)\r\n","\r\n","    for i in range(min(BATCH_SIZE, 10)):\r\n","        plt.figure(figsize=(10, 10))\r\n","        plt.subplot(1, 2, 1)       \r\n","        plt.imshow(x[i].permute(1, 2, 0))\r\n","        plt.subplot(1, 2, 2)     \r\n","        plt.imshow(y[i].squeeze(0), cmap='gray')\r\n","    break"],"outputs":[],"metadata":{"_uuid":"b4cc8587-c000-441f-af8b-c8187c3f79a7","_cell_guid":"3c2ab771-d2fe-4eea-ba80-f6e37c548f82","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:24:59.971180Z","iopub.execute_input":"2023-07-16T16:24:59.971927Z","iopub.status.idle":"2023-07-16T16:25:10.234808Z","shell.execute_reply.started":"2023-07-16T16:24:59.971889Z","shell.execute_reply":"2023-07-16T16:25:10.233651Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def save_checkpoint(state, filename):\r\n","    print(\"=> Saving checkpoint\")\r\n","    torch.save(state, filename)\r\n","\r\n","def load_checkpoint(checkpoint, model):\r\n","    print(\"=> Loading checkpoint\")\r\n","    model.load_state_dict(checkpoint[\"state_dict\"])\r\n","\r\n","def train_fn(loader, model, optimizer, loss_fn, scaler, lr_scheduler, epoch):    \r\n","    loop = tqdm(loader)\r\n","    \r\n","    for batch_idx, (data, targets) in enumerate(loop):\r\n","        data = data.to(device=DEVICE)\r\n","        targets = targets.to(device=DEVICE).unsqueeze(1)\r\n","\r\n","        # forward\r\n","        with torch.cuda.amp.autocast():\r\n","            predictions = model(data)\r\n","            loss = loss_fn(predictions, targets)\r\n","\r\n","        # backward\r\n","        optimizer.zero_grad(set_to_none=True)\r\n","        scaler.scale(loss).backward()\r\n","        scaler.step(optimizer)\r\n","        scaler.update()\r\n","        \r\n","        lr_scheduler.step()\r\n","        \r\n","        # update tqdm loop\r\n","        loop.set_postfix(loss=loss.item(), epoch=epoch, lr=optimizer.param_groups[0][\"lr\"])\r\n","\r\n","def validate_fn(loader, model, loss_fn, device=\"cuda\"):\r\n","    dice_score = 0\r\n","    loss = 0\r\n","    \r\n","    model.eval()\r\n","\r\n","    with torch.cuda.amp.autocast():\r\n","        with torch.no_grad():\r\n","            for x, y in loader:\r\n","                x = x.to(device)\r\n","                y = y.to(device).unsqueeze(1)\r\n","                \r\n","                preds = torch.sigmoid(model(x))\r\n","                preds[preds >= 0.5] = 1\r\n","\r\n","                dice_score += (2 * (preds * y).sum()) / (\r\n","                    (preds + y).sum() + 1e-8\r\n","                )\r\n","                \r\n","                loss += loss_fn(preds, y)\r\n","\r\n","    model.train()\r\n","    dice_score = dice_score / len(loader)\r\n","    loss = loss / len(loader)\r\n","    \r\n","    print(f\"Dice score\\t{dice_score:.3f}\")\r\n","    print(f\"Val loss\\t{loss:.3f}\")\r\n","    \r\n","    \r\n","    return dice_score, loss"],"outputs":[],"metadata":{"_uuid":"69710d3b-8268-452b-834b-82217b45f28a","_cell_guid":"622c1cfb-c149-4f09-baf9-ef3f2d441bb0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:25:10.236375Z","iopub.execute_input":"2023-07-16T16:25:10.237370Z","iopub.status.idle":"2023-07-16T16:25:10.251346Z","shell.execute_reply.started":"2023-07-16T16:25:10.237335Z","shell.execute_reply":"2023-07-16T16:25:10.250221Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["import torch.cuda.amp as amp\r\n","import torch.nn.functional as F\r\n","\r\n","##\r\n","# version 2: user derived grad computation\r\n","class FocalSigmoidLossFunc(torch.autograd.Function):\r\n","    '''\r\n","    compute backward directly for better numeric stability\r\n","    '''\r\n","    @staticmethod\r\n","    @amp.custom_fwd(cast_inputs=torch.float32)\r\n","    def forward(ctx, logits, label, alpha, gamma):\r\n","        #  logits = logits.float()\r\n","\r\n","        probs = torch.sigmoid(logits)\r\n","        coeff = (label - probs).abs_().pow_(gamma).neg_()\r\n","        log_probs = torch.where(logits >= 0,\r\n","                F.softplus(logits, -1, 50),\r\n","                logits - F.softplus(logits, 1, 50))\r\n","        log_1_probs = torch.where(logits >= 0,\r\n","                -logits + F.softplus(logits, -1, 50),\r\n","                -F.softplus(logits, 1, 50))\r\n","        ce_term1 = log_probs.mul_(label).mul_(alpha)\r\n","        ce_term2 = log_1_probs.mul_(1. - label).mul_(1. - alpha)\r\n","        ce = ce_term1.add_(ce_term2)\r\n","        loss = ce * coeff\r\n","\r\n","        ctx.vars = (coeff, probs, ce, label, gamma, alpha)\r\n","\r\n","        return loss\r\n","\r\n","    @staticmethod\r\n","    @amp.custom_bwd\r\n","    def backward(ctx, grad_output):\r\n","        '''\r\n","        compute gradient of focal loss\r\n","        '''\r\n","        (coeff, probs, ce, label, gamma, alpha) = ctx.vars\r\n","\r\n","        d_coeff = (label - probs).abs_().pow_(gamma - 1.).mul_(gamma)\r\n","        d_coeff.mul_(probs).mul_(1. - probs)\r\n","        d_coeff = torch.where(label < probs, d_coeff.neg(), d_coeff)\r\n","        term1 = d_coeff.mul_(ce)\r\n","\r\n","        d_ce = label * alpha\r\n","        d_ce.sub_(probs.mul_((label * alpha).mul_(2).add_(1).sub_(label).sub_(alpha)))\r\n","        term2 = d_ce.mul(coeff)\r\n","\r\n","        grads = term1.add_(term2)\r\n","        grads.mul_(grad_output)\r\n","\r\n","        return grads, None, None, None\r\n","\r\n","\r\n","class FocalLoss(nn.Module):\r\n","\r\n","    def __init__(self,\r\n","                 alpha=0.25,\r\n","                 gamma=2,\r\n","                 reduction='mean'):\r\n","        super(FocalLoss, self).__init__()\r\n","        self.alpha = alpha\r\n","        self.gamma = gamma\r\n","        self.reduction = reduction\r\n","\r\n","    def forward(self, logits, label):\r\n","        '''\r\n","        Usage is same as nn.BCEWithLogits:\r\n","            >>> criteria = FocalLossV2()\r\n","            >>> logits = torch.randn(8, 19, 384, 384)\r\n","            >>> lbs = torch.randint(0, 2, (8, 19, 384, 384)).float()\r\n","            >>> loss = criteria(logits, lbs)\r\n","        '''\r\n","        loss = FocalSigmoidLossFunc.apply(logits, label, self.alpha, self.gamma)\r\n","        if self.reduction == 'mean':\r\n","            loss = loss.mean()\r\n","        if self.reduction == 'sum':\r\n","            loss = loss.sum()\r\n","        return loss"],"outputs":[],"metadata":{"_uuid":"d2b57f37-7068-4183-ad84-04ae4eda72a8","_cell_guid":"c0ad026b-976a-46ce-badd-b6811810857a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:25:10.252616Z","iopub.execute_input":"2023-07-16T16:25:10.253477Z","iopub.status.idle":"2023-07-16T16:25:10.272493Z","shell.execute_reply.started":"2023-07-16T16:25:10.253441Z","shell.execute_reply":"2023-07-16T16:25:10.271605Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["class DilatedConvBlock(nn.Module):\r\n","    def __init__(self, ch_in, ch_out, dilation=2):\r\n","        super(DilatedConvBlock, self).__init__()\r\n","        self.conv = nn.Sequential(\r\n","            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=True),\r\n","            nn.BatchNorm2d(ch_out),\r\n","            nn.LeakyReLU(inplace=True),\r\n","            \r\n","            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=True),\r\n","            nn.BatchNorm2d(ch_out),\r\n","            nn.LeakyReLU(inplace=True),\r\n","        )\r\n","        \r\n","    def forward(self, x):\r\n","        return self.conv(x)\r\n","    \r\n","class ConvBlock(nn.Module):\r\n","    def __init__(self, ch_in, ch_out):\r\n","        super(ConvBlock, self).__init__()\r\n","        self.conv = nn.Sequential(\r\n","            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\r\n","            nn.BatchNorm2d(ch_out),\r\n","            nn.LeakyReLU(inplace=True),\r\n","            \r\n","            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\r\n","            nn.BatchNorm2d(ch_out),\r\n","            nn.LeakyReLU(inplace=True),\r\n","        )\r\n","        \r\n","    def forward(self, x):\r\n","        return self.conv(x)\r\n","\r\n","\r\n","class UpConvBlock(nn.Module):\r\n","    def __init__(self, ch_in, ch_out):\r\n","        super(UpConvBlock, self).__init__()\r\n","        self.up = nn.Sequential(\r\n","            nn.Upsample(scale_factor=2),\r\n","            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\r\n","            nn.BatchNorm2d(ch_out),\r\n","            nn.LeakyReLU(inplace=True),\r\n","        )\r\n","\r\n","    def forward(self, x):\r\n","        x = self.up(x)\r\n","        return x\r\n","\r\n","\r\n","class AttentionBlock(nn.Module):\r\n","    def __init__(self, F_g, F_l, F_int):\r\n","        super(AttentionBlock, self).__init__()\r\n","        self.W_g = nn.Sequential(\r\n","            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\r\n","            nn.BatchNorm2d(F_int)\r\n","        )\r\n","\r\n","        self.W_x = nn.Sequential(\r\n","            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\r\n","            nn.BatchNorm2d(F_int)\r\n","        )\r\n","\r\n","        self.psi = nn.Sequential(\r\n","            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\r\n","            nn.BatchNorm2d(1),\r\n","            nn.Sigmoid()\r\n","        )\r\n","\r\n","        self.relu = nn.LeakyReLU(inplace=True)\r\n","\r\n","    def forward(self, g, x):\r\n","        g1 = self.W_g(g)\r\n","        x1 = self.W_x(x)\r\n","        psi = self.relu(g1+x1)\r\n","        psi = self.psi(psi)\r\n","\r\n","        return x*psi\r\n","    \r\n","    \r\n","class UpModule(nn.Module):\r\n","    def __init__(self, feature):\r\n","        super(UpModule, self).__init__()\r\n","        self.up = UpConvBlock(feature*2, feature)\r\n","        self.attn = AttentionBlock(feature, feature, feature // 2)\r\n","        self.conv = DilatedConvBlock(feature*2, feature)\r\n","        self.act = nn.LeakyReLU(inplace=True)\r\n","\r\n","    def forward(self, x, skip_connection):\r\n","        x = self.up(x)\r\n","        skip = skip_connection\r\n","\r\n","        if x.shape != skip_connection.shape:\r\n","            x = torchvision.transforms.functional.resize(x, size=skip_connection.shape[2:])\r\n","\r\n","        skip_connection = self.attn(x, skip_connection)\r\n","\r\n","        concat_skip = torch.cat((skip_connection, x), dim=1)\r\n","        x = self.conv(concat_skip)\r\n","        \r\n","        x = torch.add(x, skip)\r\n","        x = torch.add(x, skip_connection)\r\n","        \r\n","        return self.act(x)\r\n","\r\n","class DownModule(nn.Module):\r\n","    def __init__(self, in_channels, feature):\r\n","        super(DownModule, self).__init__()\r\n","        self.down = DilatedConvBlock(in_channels, feature)\r\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\r\n","        \r\n","\r\n","    def forward(self, x):\r\n","        x = self.down(x)\r\n","        skip = x\r\n","        x = self.pool(x)\r\n","\r\n","        return x, skip\r\n","\r\n","\r\n","class AttenUNET(nn.Module):\r\n","    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256]):\r\n","        super(AttenUNET, self).__init__()\r\n","        \r\n","        self.ups = nn.ModuleList()\r\n","        self.downs = nn.ModuleList()\r\n","        \r\n","        down_outputs = 0\r\n","        \r\n","        # Down part of UNET\r\n","        for feature in features:\r\n","            self.downs.append(DownModule(in_channels, feature))\r\n","            in_channels = feature\r\n","        \r\n","                \r\n","        # Up part of UNET\r\n","        for feature in reversed(features):\r\n","            self.ups.append(UpModule(feature))\r\n","\r\n","        self.bottleneck = nn.Sequential(\r\n","            ConvBlock(features[-1], features[-1]*2),\r\n","            ConvBlock(features[-1]*2, features[-1]*2),\r\n","        )\r\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\r\n","\r\n","\r\n","    def forward(self, x):\r\n","        #x = nn.functional.pad(x, (PADDING, PADDING), value=1) # White padding\r\n","        skip_connections = []\r\n","\r\n","        for down in self.downs:\r\n","            x, skip = down(x)\r\n","            skip_connections.append(skip)\r\n","\r\n","        x = self.bottleneck(x)\r\n","        skip_connections = skip_connections[::-1]\r\n","        \r\n","        \r\n","        for i, up in enumerate(self.ups):\r\n","            skip_connection = skip_connections[i]\r\n","            x = self.ups[i](x, skip_connection)\r\n","        \r\n","        y = self.final_conv(x)\r\n","        #y = nn.functional.interpolate(y, size=(HEIGHT, WIDTH)) # Reverse the padding\r\n","        return y"],"outputs":[],"metadata":{"_uuid":"289ac763-9245-4b27-ac15-306a2147c0cf","_cell_guid":"9caf20b0-5bc0-4db6-94f4-1dfbae8c2126","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:25:10.273776Z","iopub.execute_input":"2023-07-16T16:25:10.275662Z","iopub.status.idle":"2023-07-16T16:25:10.306178Z","shell.execute_reply.started":"2023-07-16T16:25:10.275633Z","shell.execute_reply":"2023-07-16T16:25:10.305255Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model = AttenUNET(3, 1).to(DEVICE)\r\n","#model = torch.compile(model) # Faster\r\n","\r\n","loss_fn = FocalLoss(gamma=2)\r\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n","num_steps = (len(train_loader) * NUM_EPOCHS)\r\n","ex = lambda x: 0.999 ** x\r\n","lr_lambda = lambda x: ex(x) if ex(x) > 1e-2 else 1e-2\r\n","lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\r\n","scaler = torch.cuda.amp.GradScaler()"],"outputs":[],"metadata":{"_uuid":"3c4f32fd-4472-4bb8-bb98-d86354c87fee","_cell_guid":"62c6fb9c-fd18-40eb-9292-32d95043a9c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:25:10.307728Z","iopub.execute_input":"2023-07-16T16:25:10.308089Z","iopub.status.idle":"2023-07-16T16:25:10.450872Z","shell.execute_reply.started":"2023-07-16T16:25:10.308057Z","shell.execute_reply":"2023-07-16T16:25:10.449844Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["max_score = 0.0\r\n","\r\n","for epoch in range(NUM_EPOCHS):\r\n","    train_fn(train_loader, model, optimizer, loss_fn, scaler, lr_scheduler, epoch)\r\n","\r\n","    # check accuracy\r\n","    dice_score, loss = validate_fn(val_loader, model, loss_fn, device=DEVICE)\r\n","    \r\n","    if dice_score > max_score:\r\n","        max_score = dice_score\r\n","        \r\n","        checkpoint = {\r\n","            \"state_dict\": model.state_dict(),\r\n","        }\r\n","        save_checkpoint(checkpoint, \"attn-unet-best.pth.tar\")"],"outputs":[],"metadata":{"_uuid":"593fdcb9-ad5b-477b-978f-5bdabc9855ea","_cell_guid":"049327cc-9ee3-455c-8aa9-85db02441a05","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-16T16:25:11.752995Z","iopub.execute_input":"2023-07-16T16:25:11.753262Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["model.eval()\r\n","\r\n","for x, y in val_loader:\r\n","    with torch.no_grad():\r\n","        x = x.to(DEVICE)\r\n","        y = y.to(DEVICE).unsqueeze(1)\r\n","        preds = torch.sigmoid(model(x))\r\n","        preds[preds >= 0.5] = 1\r\n","        \r\n","        for i in range(BATCH_SIZE):\r\n","            plt.figure(figsize=(10, 10))\r\n","            plt.subplot(1, 2, 1)\r\n","            plt.imshow(x[i].cpu().permute(1, 2, 0))\r\n","            plt.imshow(y[i].cpu().squeeze(0), cmap='gray', alpha=0.5)\r\n","            \r\n","            plt.subplot(1, 2, 2)\r\n","            plt.imshow(x[i].cpu().permute(1, 2, 0))\r\n","            plt.imshow(preds[i].cpu().squeeze(0), cmap='gray', alpha=0.5)\r\n","        break\r\n","model.train()"],"outputs":[],"metadata":{"_uuid":"b0935d0a-e4f2-48ce-802b-afbcab924031","_cell_guid":"b939452d-3b7f-4575-862e-1facf7545882","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true}}]}